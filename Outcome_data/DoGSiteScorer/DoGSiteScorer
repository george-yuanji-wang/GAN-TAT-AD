
import requests
import json
import time


# Read the protein names from the text file
with open(r'C:\Users\Srisharan\OneDrive\Desktop\ISEF\ISEF-2023\Outcome_data\All_unknown_predicted_proteins.csv', 'r') as file:
    next(file)  # Skip the header line
    protein_names = [line.split(',')[0].strip() for line in file][:5]  # Limit to the first 5 names

datasss = {}
# Loop through each of the first 5 protein names
for uniprot_name in protein_names:

    # Define the JSON payload
    payload = {
        "dogsite": {
            "pdbCode": f'{uniprot_name}',
            "analysisDetail": "1",
            "bindingSitePredictionGranularity": "1",
            "ligand": "",
            "chain": ""
        }
    }

    # Define headers
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json"
    }

    # Define the URL
    url = "https://proteins.plus/api/dogsite_rest"

    # Send POST request
    response = requests.post(url, json=payload, headers=headers)

    status_code = None

    # Keep looping until status code is 200
    while status_code != 200:
        # Send POST request
        response = requests.post(url, json=payload, headers=headers)
        # Get status code from the response
        status_code = response.status_code
        # Check if status code is not 200
        if status_code != 200:
            print("Waiting for status code 200. Current status code:", status_code)
            # Wait for 5 seconds before checking again
            time.sleep(5)

    # Once the status code is 200, parse and print the response
    response_json = response.json()
    
    new_url = response_json["location"]

    job_id = new_url.split("/")[-1]

    
    status_code = None
    while status_code != 200:
        response_two = requests.get(new_url)
        stats_code = response_two.status_code

        if status_code != 200:
            time.sleep(5)

    result_table = response_two.json()['result_table']

    data = requests.get(result_table).text

    rows = data.strip().split('\n')
    headers = rows[0].split()

    data = {}

    for row in rows[1:]:
        values = row.split()
        row_dict = {}
        i = 0

        for header, value in zip(header, values):

            if i > 3:
                row_dict[header] = value

            i += 1

        data[values[0]] = row_dict

    datasss[uniprot_name] = data

with open(r'/Users/michelle/jupyter/ISEF/Outcome_data/DoGSiteScorer/DoGSiteScorer_data.json', 'w') as json_file:
    json.dump(datasss, json_file, indent=4)






