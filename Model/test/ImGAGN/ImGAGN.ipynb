{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from igraph import Graph\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from sklearn.metrics.cluster import (v_measure_score, homogeneity_score, completeness_score)\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.parameter import Parameter\n",
    "import scipy.sparse as sp\n",
    "from torch.nn.modules.module import Module\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import math\n",
    "import neptune\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[7392, 26], edge_index=[2, 49502], y=[7392])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\io\\files.py:295: RuntimeWarning: Could not add vertex ids, there is already an 'id' vertex attribute. at src/io/graphml.c:492\n",
      "  return reader(f, *args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "node_list_path = r'C:\\Users\\George\\Desktop\\ISEF-2023\\Datas\\Node list\\back up\\current_protein_Signal+meta+targets.json'\n",
    "with open(node_list_path, 'r') as file:\n",
    "    node_list = json.load(file)\n",
    "graph = r'C:\\Users\\George\\Desktop\\ISEF-2023\\Network construction\\PPI_homo_graph_features_loaded.graphml'\n",
    "# Create an igraph object\n",
    "PPI_graph = ig.Graph.Load(graph, format='graphml')\n",
    "\n",
    "feature_keys = [\n",
    "    \"Indegree\", \"Outdegree\", \"Closeness\", \"Betweenness\", \"Pagerank\", \"Cluster_coefficients\",\n",
    "    \"Nearest_Neighbor_Degree\", \"Similarity\", \"Subunit\", \"Transmembrane\",\n",
    "    \"Catalytic_activity\", \"Interaction\", \"Tissue_Specificity\", \"Disease\",\n",
    "    \"Sequence_conflict\", \"Modified_residue\", \"Function\", \"Binding_site\",\n",
    "    \"Natural_variant\", \"Alternative_products\", \"Subcellular_location\",\n",
    "    \"Active_site\", \"Disulfide_bond\", \"Mutagenesis\", \"PTM\", \"STP_involvement\"\n",
    "]\n",
    "\n",
    "features = torch.tensor([\n",
    "    PPI_graph.vs[key] for key in feature_keys\n",
    "], dtype=torch.float).t()\n",
    "\n",
    "edge_indices = torch.tensor(PPI_graph.get_edgelist(), dtype=torch.long).t()\n",
    "\n",
    "# Assuming you have a label attribute in your graph\n",
    "labels = torch.tensor(PPI_graph.vs[\"label\"], dtype=torch.float)\n",
    "\n",
    "# Create a PyTorch Geometric Data object\n",
    "data_ = Data(x=features, edge_index=edge_indices, y=labels)\n",
    "\n",
    "print(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[7392, 26], y=[7392], val_pos_edge_index=[2, 1237], test_pos_edge_index=[2, 2474], train_pos_edge_index=[2, 42062], train_neg_adj_mask=[7392, 7392], val_neg_edge_index=[2, 1237], test_neg_edge_index=[2, 2474])\n"
     ]
    }
   ],
   "source": [
    "data_ = train_test_split_edges(data_)\n",
    "print(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def add_edges(adj_real, adj_new):\n",
    "    adj = adj_real+adj_new\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    return adj\n",
    "\n",
    "def accuracy(output, labels, output_AUC):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "\n",
    "\n",
    "    recall = recall_score(labels.cpu().numpy(), preds.cpu().numpy(), zero_division=0)\n",
    "    f1_score_ = f1_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "    AUC = roc_auc_score(labels.cpu().numpy(), output_AUC.detach().cpu().numpy())\n",
    "    acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "    precision = precision_score(labels.cpu().numpy(), preds.cpu().numpy(), zero_division=0)\n",
    "    return recall, f1_score_, AUC, acc, precision\n",
    "\n",
    "def load_data(data, ratio_generated):\n",
    "    print('Processing graph data...')\n",
    "\n",
    "    # Extract features and labels from the PyTorch Geometric Data object\n",
    "    features = data.x\n",
    "    labels = data.y\n",
    "\n",
    "    # Convert train and test edge indices to standard numpy arrays for processing\n",
    "    idx_train = data.train_pos_edge_index[0].numpy()\n",
    "    idx_test = data.test_pos_edge_index[0].numpy()\n",
    "\n",
    "    # The rest of the code remains largely the same as in your original function\n",
    "    majority = np.array([x for x in idx_train if labels[x] == 0])\n",
    "    minority = np.array([x for x in idx_train if labels[x] == 1])\n",
    "\n",
    "    num_minority = minority.shape[0]\n",
    "    num_majority = majority.shape[0]\n",
    "    print(\"Number of majority: \", num_majority)\n",
    "    print(\"Number of minority: \", num_minority)\n",
    "\n",
    "    generate_node = []\n",
    "    generate_label = []\n",
    "    for i in range(len(labels), len(labels) + int(ratio_generated * num_majority) - num_minority):\n",
    "        generate_node.append(i)\n",
    "        generate_label.append(1)\n",
    "    idx_train = np.hstack((idx_train, np.array(generate_node)))\n",
    "\n",
    "    minority_test = np.array([x for x in idx_test if labels[x] == 1])\n",
    "    minority_all = np.hstack((minority, minority_test))\n",
    "\n",
    "    labels = np.hstack((labels, np.array(generate_label)))\n",
    "\n",
    "    # Construct adjacency matrix from PyTorch Geometric Data\n",
    "    edge_index = data.train_pos_edge_index.numpy()\n",
    "    adj_real = sp.coo_matrix((np.ones(edge_index.shape[1]), (edge_index[0], edge_index[1])), \n",
    "                             shape=(len(labels), len(labels)), dtype=np.float32)\n",
    "\n",
    "    adj = adj_real + adj_real.T.multiply(adj_real.T > adj_real) - adj_real.multiply(adj_real.T > adj_real)\n",
    "\n",
    "    # Normalizing features and adjacency matrix\n",
    "    features = normalize(sp.csr_matrix(features))\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labels)\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    generate_node = torch.LongTensor(np.array(generate_node))\n",
    "    minority = torch.LongTensor(minority)\n",
    "    majority = torch.LongTensor(majority)\n",
    "    minority_all = torch.LongTensor(minority_all)\n",
    "\n",
    "    return adj, adj_real, features, labels, idx_train, idx_test, generate_node, minority, majority, minority_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_data(ratio_generated, path=\"../dataset/citeseer/\", dataset=\"citeseer\"):\\n    print(\\'Loading {} dataset...\\'.format(dataset))\\n\\n    idx_features_labels = np.genfromtxt(\"{}features.{}\".format(path, dataset),\\n                                        dtype=np.float32)\\n    features = sp.csr_matrix(idx_features_labels[:, 0:-1], dtype=np.float32)\\n    labels = idx_features_labels[:, -1]\\n\\n    idx_train = np.genfromtxt(\"{}train.{}\".format(path, dataset),\\n                              dtype=np.int32).squeeze()\\n\\n    idx_test = np.genfromtxt(\"{}test.{}\".format(path, dataset),\\n                             dtype=np.int32).squeeze()\\n\\n    majority = np.array([x for x in idx_train if labels[x] == 0])\\n    minority = np.array([x for x in idx_train if labels[x] == 1])\\n\\n    num_minority = minority.shape[0]\\n    num_majority = majority.shape[0]\\n    print(\"Number of majority: \", num_majority)\\n    print(\"Number of minority: \", num_minority)\\n\\n    generate_node = []\\n    generate_label=[]\\n    for i in range(labels.shape[0], labels.shape[0]+int(ratio_generated*num_majority)-num_minority):\\n        generate_node.append(i)\\n        generate_label.append(1)\\n    idx_train= np.hstack((idx_train, np.array(generate_node)))\\n    print(idx_train.shape)\\n\\n    minority_test = np.array([x for x in idx_test if labels[x] == 1])\\n    minority_all = np.hstack((minority, minority_test))\\n\\n\\n    labels= np.hstack((labels, np.array(generate_label)))\\n\\n\\n    edges = np.genfromtxt(\"{}edges.{}\".format(path, dataset),\\n                                    dtype=np.int32)\\n\\n    adj_real = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\\n                        shape=(labels.shape[0], labels.shape[0]),\\n                        dtype=np.float32)\\n\\n    adj = adj_real + adj_real.T.multiply(adj_real.T > adj_real) - adj_real.multiply(adj_real.T > adj_real)\\n\\n    features = normalize(features)\\n    adj = normalize(adj + sp.eye(adj.shape[0]))\\n\\n    features = torch.FloatTensor(np.array(features.todense()))\\n    labels = torch.LongTensor(labels)\\n    adj = sparse_mx_to_torch_sparse_tensor(adj)\\n\\n    idx_train = torch.LongTensor(idx_train)\\n    idx_test = torch.LongTensor(idx_test)\\n    generate_node=torch.LongTensor(np.array(generate_node))\\n    minority = torch.LongTensor(minority)\\n    majority = torch.LongTensor(majority)\\n    minority_all = torch.LongTensor(minority_all)\\n\\n    return adj, adj_real,features, labels, idx_train, idx_test, generate_node, minority, majority, minority_all\\n\\n#######################################\\n#######################################\\n#######################################\\n#######################################\\n#######################################\\n#######################################'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####\n",
    "#  Orignal dataset\n",
    "####\n",
    "'''\n",
    "def load_data(ratio_generated, path=\"../dataset/citeseer/\", dataset=\"citeseer\"):\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}features.{}\".format(path, dataset),\n",
    "                                        dtype=np.float32)\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 0:-1], dtype=np.float32)\n",
    "    labels = idx_features_labels[:, -1]\n",
    "\n",
    "    idx_train = np.genfromtxt(\"{}train.{}\".format(path, dataset),\n",
    "                              dtype=np.int32).squeeze()\n",
    "\n",
    "    idx_test = np.genfromtxt(\"{}test.{}\".format(path, dataset),\n",
    "                             dtype=np.int32).squeeze()\n",
    "\n",
    "    majority = np.array([x for x in idx_train if labels[x] == 0])\n",
    "    minority = np.array([x for x in idx_train if labels[x] == 1])\n",
    "\n",
    "    num_minority = minority.shape[0]\n",
    "    num_majority = majority.shape[0]\n",
    "    print(\"Number of majority: \", num_majority)\n",
    "    print(\"Number of minority: \", num_minority)\n",
    "\n",
    "    generate_node = []\n",
    "    generate_label=[]\n",
    "    for i in range(labels.shape[0], labels.shape[0]+int(ratio_generated*num_majority)-num_minority):\n",
    "        generate_node.append(i)\n",
    "        generate_label.append(1)\n",
    "    idx_train= np.hstack((idx_train, np.array(generate_node)))\n",
    "    print(idx_train.shape)\n",
    "\n",
    "    minority_test = np.array([x for x in idx_test if labels[x] == 1])\n",
    "    minority_all = np.hstack((minority, minority_test))\n",
    "\n",
    "\n",
    "    labels= np.hstack((labels, np.array(generate_label)))\n",
    "\n",
    "\n",
    "    edges = np.genfromtxt(\"{}edges.{}\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "\n",
    "    adj_real = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    adj = adj_real + adj_real.T.multiply(adj_real.T > adj_real) - adj_real.multiply(adj_real.T > adj_real)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labels)\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    generate_node=torch.LongTensor(np.array(generate_node))\n",
    "    minority = torch.LongTensor(minority)\n",
    "    majority = torch.LongTensor(majority)\n",
    "    minority_all = torch.LongTensor(minority_all)\n",
    "\n",
    "    return adj, adj_real,features, labels, idx_train, idx_test, generate_node, minority, majority, minority_all\n",
    "\n",
    "#######################################\n",
    "#######################################\n",
    "#######################################\n",
    "#######################################\n",
    "#######################################\n",
    "#######################################'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim // 2, output_dim, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, generate_node, min_node):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.gc3 = GraphConvolution(nhid, 2)\n",
    "        self.attention = Attention(nfeat*2, 1)\n",
    "        self.generate_node = generate_node\n",
    "        self.min_node = min_node\n",
    "        self.dropout = dropout\n",
    "        self.eps = 1e-10\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x1 = self.gc2(x, adj)\n",
    "        x2 = self.gc3(x, adj)\n",
    "        return F.log_softmax(x1, dim=1), F.log_softmax(x2, dim=1), F.softmax(x1, dim=1)[:,-1]\n",
    "\n",
    "    def get_embedding(self,x , adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = torch.spmm(adj, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,  dim):\n",
    "        super(Generator, self).__init__( )\n",
    "\n",
    "        self.fc1 = nn.Linear(100, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, dim)\n",
    "        self.fc4 = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = (x+1)/2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graph data...\n",
      "Number of majority:  41319\n",
      "Number of minority:  743\n",
      "torch.Size([14912, 14912]) (14912, 14912) torch.Size([7392, 26]) torch.Size([14912]) torch.Size([49582]) torch.Size([2474]) torch.Size([7520]) torch.Size([743]) torch.Size([41319]) torch.Size([814])\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.2\n",
    "hidden = 128\n",
    "dropout = 0.5\n",
    "lr = 0.0009\n",
    "weight_decay = 0.003\n",
    "fastmode = False\n",
    "no_cuda = False\n",
    "num= 10\n",
    "seed= 42\n",
    "epochs_gen = 10\n",
    "epochs = 10\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "adj, adj_real, features, labels, idx_temp, idx_test, generate_node, minority, majority, minority_all = load_data(data_, ratio)\n",
    "\n",
    "print(adj.shape, adj_real.shape, features.shape, labels.shape, idx_temp.shape, idx_test.shape, generate_node.shape, minority.shape, majority.shape, minority_all.shape)\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "    nhid=hidden,\n",
    "    nclass=labels.max().item() + 1,\n",
    "    dropout=dropout,\n",
    "    generate_node= generate_node,\n",
    "    min_node = minority)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "num_false = labels.shape[0]- features.shape[0]\n",
    "model_generator = Generator(minority_all.shape[0])\n",
    "optimizer_G = torch.optim.Adam(model_generator.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "max_recall = 0\n",
    "test_recall = 0\n",
    "test_f1 = 0\n",
    "test_AUC = 0\n",
    "test_acc=0\n",
    "test_pre =0\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_temp = idx_temp.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "    model_generator.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, adj):\n",
    "    global max_recall, test_recall, test_f1, test_AUC, test_acc, test_pre\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output, output_gen, output_AUC = model(features, adj)\n",
    "\n",
    "    print(f\"Size of features: {features.shape}\")\n",
    "    print(f\"Size of output: {output.shape}\")\n",
    "    print(f\"Size of output_gen: {output_gen.shape}\")\n",
    "    print(f\"Size of output_AUC: {output_AUC.shape}\")\n",
    "    print(f\"Size of labels: {labels.shape}\")\n",
    "    print(f\"Size of idx_train: {idx_train.shape}\")\n",
    "    print(f\"Size of num_flase: {num_false}\")\n",
    "\n",
    "    labels_true = torch.cat((torch.LongTensor(num_real).fill_(0), torch.LongTensor(num_false).fill_(1)))\n",
    "\n",
    "    print(f\"Size of labels_true: {labels_true.size()}\")\n",
    "\n",
    "    if cuda:\n",
    "        labels_true=labels_true.cuda()\n",
    "\n",
    "    loss_dis = - euclidean_dist(features[minority], features[majority]).mean()\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train]) + F.nll_loss(output_gen[idx_train], labels_true) + loss_dis\n",
    "\n",
    "    print(f\"Size of output[idx_train]: {output[idx_train].shape}\")\n",
    "    print(f\"Size of labels[idx_train]: {labels[idx_train].shape}\")\n",
    "    print(f\"Size of output_gen[idx_train]: {output_gen[idx_train].shape}\")\n",
    "\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if not fastmode:\n",
    "        model.eval()\n",
    "        output, output_gen, output_AUC = model(features, adj)\n",
    "\n",
    "\n",
    "    recall_val, f1_val, AUC_val, acc_val, pre_val = accuracy(output[idx_val], labels[idx_val], output_AUC[idx_val])\n",
    "    recall_train, f1_train, AUC_train, acc_train, pre_train = accuracy(output[idx_val], labels[idx_val], output_AUC[idx_val])\n",
    "\n",
    "    if max_recall < (recall_val + acc_val)/2:\n",
    "        output, output_gen, output_AUC = model(features, adj)\n",
    "        recall_tmp, f1_tmp, AUC_tmp, acc_tmp, pre_tmp = accuracy(output[idx_test], labels[idx_test], output_AUC[idx_test])\n",
    "        test_recall = recall_tmp\n",
    "        test_f1 = f1_tmp\n",
    "        test_AUC = AUC_tmp\n",
    "        test_acc = acc_tmp\n",
    "        test_pre = pre_tmp\n",
    "        max_recall = (recall_val + acc_val)/2\n",
    "\n",
    "    return recall_val, f1_val, acc_val, recall_train, f1_train, acc_train\n",
    "\n",
    "\n",
    "def euclidean_dist(x, y):\n",
    "    m, n = x.size(0), y.size(0)\n",
    "    xx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\n",
    "    yy = torch.pow(y, 2).sum(1, keepdim=True).expand(n, m).t()\n",
    "    dist = xx + yy\n",
    "    dist.addmm_(1, -2, x, y.t())\n",
    "    dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max generate_node index: tensor(14911)\n",
      "Max minority_all index: tensor(7287)\n",
      "Shape of labels: torch.Size([14912])\n",
      "Size of features: torch.Size([14912, 26])\n",
      "Size of output: torch.Size([14912, 2])\n",
      "Size of output_gen: torch.Size([14912, 2])\n",
      "Size of output_AUC: torch.Size([14912])\n",
      "Size of labels: torch.Size([14912])\n",
      "Size of idx_train: torch.Size([45377])\n",
      "Size of num_flase: 7520\n",
      "Size of labels_true: torch.Size([8233])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (45377) to match target batch_size (8233).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [157], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     optimizer_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 57\u001b[0m         recall_val, f1_val, acc_val, recall_train, f1_train, acc_train \u001b[38;5;241m=\u001b[39m train(torch\u001b[38;5;241m.\u001b[39mcat((features, gen_imgs1\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdetach()),\u001b[38;5;241m0\u001b[39m), adj_new)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%04d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch_gen \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_recall=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(recall_train), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_f1=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(f1_train),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(acc_train),\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_recall=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(recall_val), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_f1=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(f1_val),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(acc_val))\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Recall: \u001b[39m\u001b[38;5;124m\"\u001b[39m, test_recall)\n",
      "Cell \u001b[1;32mIn [156], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(features, adj)\u001b[0m\n\u001b[0;32m     20\u001b[0m     labels_true\u001b[38;5;241m=\u001b[39mlabels_true\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     22\u001b[0m loss_dis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m euclidean_dist(features[minority], features[majority])\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m---> 23\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output[idx_train], labels[idx_train]) \u001b[38;5;241m+\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_gen\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_true\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m loss_dis\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of output[idx_train]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput[idx_train]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of labels[idx_train]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[idx_train]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2729\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2728\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (45377) to match target batch_size (8233)."
     ]
    }
   ],
   "source": [
    "for epoch_gen in range(epochs_gen):\n",
    "    part = epoch_gen % num\n",
    "    range_val_maj = range(int(part*len(majority)/num), int((part+1)*len(majority)/num))\n",
    "    range_val_min = range(int(part * len(minority) / num), int((part + 1) * len(minority) / num))\n",
    "\n",
    "    range_train_maj = list(range(0,int(part*len(majority)/num)))+ list(range(int((part+1)*len(majority)/num),len(majority)))\n",
    "    range_train_min = list(range(0,int(part*len(minority)/num)))+ list(range(int((part+1)*len(minority)/num),len(minority)))\n",
    "\n",
    "    idx_val = torch.cat((majority[range_val_maj], minority[range_val_min]))\n",
    "    idx_train = torch.cat((majority[range_train_maj], minority[range_train_min]))\n",
    "    idx_train = torch.cat((idx_train, generate_node))\n",
    "    num_real = features.shape[0] - len(idx_test) -len(idx_val)\n",
    "\n",
    "    # Train model\n",
    "    model_generator.train()\n",
    "    optimizer_G.zero_grad()\n",
    "    z = Variable(torch.FloatTensor(np.random.normal(0, 1, (generate_node.shape[0], 100))))\n",
    "    if cuda:\n",
    "        z=z.cuda()\n",
    "\n",
    "    adj_min = model_generator(z)\n",
    "    gen_imgs1 = torch.mm(F.softmax(adj_min[:,0:minority.shape[0]], dim=1), features[minority])\n",
    "    gen_imgs1_all = torch.mm(F.softmax(adj_min, dim=1), features[minority_all])\n",
    "\n",
    "    matr = F.softmax(adj_min[:,0:minority.shape[0]], dim =1).data.cpu().numpy()\n",
    "    pos=np.where(matr>1/matr.shape[1])\n",
    "\n",
    "    print(\"Max generate_node index:\", generate_node.max())\n",
    "    print(\"Max minority_all index:\", minority_all.max())\n",
    "    print(\"Shape of labels:\", labels.shape)\n",
    "\n",
    "    adj_temp = sp.coo_matrix((np.ones(pos[0].shape[0]),(generate_node[pos[0]].numpy(), minority_all[pos[1]].numpy())),\n",
    "                             shape=(labels.shape[0], labels.shape[0]),\n",
    "                             dtype=np.float32)\n",
    "\n",
    "    adj_new = add_edges(adj_real, adj_temp)\n",
    "    if cuda:\n",
    "        adj_new=adj_new.cuda()\n",
    "\n",
    "    t_total = time.time()\n",
    "    # model.eval()\n",
    "    output, output_gen, output_AUC = model(torch.cat((features, gen_imgs1.data),0), adj)\n",
    "\n",
    "    labels_true = torch.LongTensor(num_false).fill_(0)\n",
    "    labels_min = torch.LongTensor(num_false).fill_(1)\n",
    "    if cuda:\n",
    "        labels_true = labels_true.cuda()\n",
    "        labels_min = labels_min.cuda()\n",
    "\n",
    "    g_loss = F.nll_loss(output_gen[generate_node], labels_true) \\\n",
    "             + F.nll_loss(output[generate_node], labels_min) \\\n",
    "             + euclidean_dist(features[minority], gen_imgs1).mean()\n",
    "    g_loss.backward()\n",
    "    optimizer_G.step()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        recall_val, f1_val, acc_val, recall_train, f1_train, acc_train = train(torch.cat((features, gen_imgs1.data.detach()),0), adj_new)\n",
    "    print(\"Epoch:\", '%04d' % (epoch_gen + 1),\n",
    "        \"train_recall=\", \"{:.5f}\".format(recall_train), \"train_f1=\", \"{:.5f}\".format(f1_train),\"train_acc=\", \"{:.5f}\".format(acc_train),\n",
    "        \"val_recall=\", \"{:.5f}\".format(recall_val), \"val_f1=\", \"{:.5f}\".format(f1_val),\"val_acc=\", \"{:.5f}\".format(acc_val))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test Recall: \", test_recall)\n",
    "print(\"Test Accuracy: \", test_acc)\n",
    "print(\"Test F1: \", test_f1)\n",
    "print(\"Test precision: \", test_pre)\n",
    "print(\"Test AUC: \", test_AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
