{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nni\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import models\n",
    "import utils\n",
    "import data_load\n",
    "import QLearning\n",
    "\n",
    "import random\n",
    "seed=42\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "\n",
    "    global last_k\n",
    "    global current_k\n",
    "    global last_acc\n",
    "    global current_acc\n",
    "    global action\n",
    "    global k_record\n",
    "    global Endepoch\n",
    "\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    decoder.train()\n",
    "    pairdis.train()\n",
    "    clusterdis.train()\n",
    "\n",
    "    optimizer_en.zero_grad()\n",
    "    optimizer_cls.zero_grad()\n",
    "    optimizer_de.zero_grad()\n",
    "\n",
    "    if param['model'] == 'sem':\n",
    "        embed, loss_sem = encoder(features, adj)\n",
    "    else:\n",
    "        embed = encoder(features, adj)\n",
    "\n",
    "    if param['setting'] == 'joint' or param['setting'] == 'pre-train' or param['setting'] == 'fine-tune':\n",
    "        # Feature Mixup, Label Mixup, and Edge Mixup in the semantic relation space\n",
    "        embed, labels_new, idx_train_new, adj_up = utils.mixup(embed, labels, idx_train, adj=adj.detach(), up_scale=param['up_scale'], im_class_num=param['num_im_class'], scale=current_k)\n",
    "\n",
    "        n_num = labels.shape[0]\n",
    "        adj_rec = decoder(embed)\n",
    "        # Three Losses for training edge predictor\n",
    "        loss_rec = utils.adj_mse_loss(adj_rec[:n_num, :][:, :n_num], adj.detach(), param)\n",
    "        loss_dis = pairdis(embed[:n_num])\n",
    "        loss_clu = clusterdis(embed[:n_num])\n",
    "\n",
    "        # Obtain threshold binary edges or soft continuous edges\n",
    "        if param['mode'] == 'discrete_edge':\n",
    "            adj_new = copy.deepcopy(adj_rec.detach())\n",
    "            threshold = 0.5\n",
    "            adj_new[adj_new < threshold] = 0.0\n",
    "            adj_new[adj_new >= threshold] = 1.0\n",
    "        else:\n",
    "            adj_new = adj_rec\n",
    "\n",
    "        adj_new = torch.mul(adj_up, adj_new)\n",
    "        adj_new[:n_num, :][:, :n_num] = adj.detach()\n",
    "\n",
    "        if param['mode'] == 'discrete_edge':\n",
    "            adj_new = adj_new.detach()\n",
    "\n",
    "    elif param['setting'] == 'embed_smote':\n",
    "        embed, labels_new, idx_train_new = utils.mixup(embed, labels, idx_train, up_scale=param['up_scale'], im_class_num=param['num_im_class'])\n",
    "        adj_new = adj\n",
    "\n",
    "    else:\n",
    "        labels_new = labels\n",
    "        idx_train_new = idx_train\n",
    "        adj_new = adj\n",
    "\n",
    "    output = classifier(embed, adj_new)\n",
    "\n",
    "    # The Re-weight method assign larger weight to losses of samples on minority classes\n",
    "    if param['setting'] == 're-weight':\n",
    "        weight = features.new((labels.max().item() + 1)).fill_(1)\n",
    "        c_largest = labels.max().item()\n",
    "        avg_number = int(idx_train.shape[0] / (c_largest + 1))\n",
    "\n",
    "        for i in range(param['num_im_class']):\n",
    "            if param['up_scale'] != 0:\n",
    "                weight[c_largest-i] = 1 + param['up_scale']\n",
    "            else:\n",
    "                chosen = idx_train[(labels == (c_largest - i))[idx_train]]\n",
    "                c_up_scale = int(avg_number / chosen.shape[0]) - 1\n",
    "                if c_up_scale >= 0:\n",
    "                    weight[c_largest-i] = 1 + c_up_scale\n",
    "        loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new], weight=weight)\n",
    "    else:\n",
    "        loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new])\n",
    "\n",
    "    acc_train, auc_train, f1_train, positive_precision_train, positive_recall_train, positive_f1_train, conf_matrix = utils.evaluation(output[idx_train], labels[idx_train])\n",
    "\n",
    "    if param['setting'] == 'joint':\n",
    "        loss = loss_train + loss_rec\n",
    "        if param['dis_weight'] != 0:\n",
    "            loss += loss_dis * param['dis_weight']\n",
    "        if param['clu_weight'] != 0:\n",
    "            loss += loss_clu * param['clu_weight']\n",
    "        if param['model'] == 'sem':\n",
    "            loss += loss_sem\n",
    "        else:\n",
    "            loss_sem = loss_train\n",
    "        loss.backward()\n",
    "        optimizer_en.step()\n",
    "        optimizer_cls.step()\n",
    "        optimizer_de.step()\n",
    "\n",
    "        if epoch >= 50 and (not QLearning.isTerminal(k_record)):\n",
    "            last_k, current_k, action = QLearning.Run_QL(env, RL, current_acc=current_acc, last_acc=last_acc, last_k=last_k, current_k=current_k, action=action)\n",
    "            k_record.append(current_k)\n",
    "            Endepoch = epoch\n",
    "        else:\n",
    "            k_record.append(current_k)\n",
    "\n",
    "    # Perform joint training\n",
    "    elif param['setting'] == 'pre-train':\n",
    "        loss = loss_rec + 0 * loss_train\n",
    "        if param['dis_weight'] != 0:\n",
    "            loss += loss_dis * param['dis_weight']\n",
    "        if param['clu_weight'] != 0:\n",
    "            loss += loss_clu * param['clu_weight']\n",
    "        if param['model'] == 'sem':\n",
    "            loss += loss_sem\n",
    "        else:\n",
    "            loss_sem = loss_train\n",
    "        loss.backward()\n",
    "        optimizer_en.step()\n",
    "        optimizer_cls.step()\n",
    "        optimizer_de.step()\n",
    "\n",
    "    # Perform pre-training\n",
    "    elif param['setting'] == 'fine-tune':\n",
    "        loss = loss_train\n",
    "        if param['model'] != 'sem':\n",
    "            loss_sem = loss_train\n",
    "        loss.backward()\n",
    "        optimizer_en.step()\n",
    "        optimizer_de.zero_grad()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        if epoch >= 50 and (not QLearning.isTerminal(k_record, delta_k=param['delta_k'])):\n",
    "            last_k, current_k, action = QLearning.Run_QL(env, RL, current_acc=current_acc, last_acc=last_acc, last_k=last_k, current_k=current_k, action=action)\n",
    "            k_record.append(current_k)\n",
    "            Endepoch = epoch\n",
    "        else:\n",
    "            k_record.append(current_k)\n",
    "\n",
    "    # Perform fine-tuning or training with original settings\n",
    "    else:\n",
    "        loss = loss_train\n",
    "        loss_rec = loss_train\n",
    "        loss_dis = loss_train\n",
    "        loss_clu = loss_train\n",
    "        if param['model'] == 'sem':\n",
    "            loss += loss_sem\n",
    "        else:\n",
    "            loss_sem = loss_train\n",
    "        loss.backward()  \n",
    "        optimizer_en.step()     \n",
    "        optimizer_cls.step()\n",
    "\n",
    "    loss_val = F.cross_entropy(output[idx_val], labels[idx_val])\n",
    "    acc_val, auc_val, macro_F, positive_precision, positive_recall, f1_val, conf_matrix = utils.evaluation(output[idx_val], labels[idx_val])\n",
    "    last_acc = current_acc\n",
    "    current_acc = f1_val\n",
    "\n",
    "    print('\\033[0;30;46m Epoch: {:04d}, loss_train: {:.4f}, loss_rec: {:.4f}, loss_dis: {:.4f}, loss_clu: {:.4f}, loss_sem: {:.4f}, acc_train: {:.4f}, loss_val: {:.4f}, acc_val: {:.4f}, auc_val: {:.4f}, macro_F: {:.4f}, positive_precision: {:.4f}, positive_recall: {:.4f}, f1_val: {:.4f}\\033[0m'.format(\n",
    "                        epoch, loss_train.item(), loss_rec.item(), loss_dis.item(), loss_clu.item(), loss_sem.item(), acc_train, loss_val.item(), acc_val, auc_val, macro_F, positive_precision, positive_recall, f1_val))\n",
    "\n",
    "    return f1_val\n",
    "\n",
    "def test(epoch):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    decoder.eval()\n",
    "    pairdis.eval()\n",
    "    clusterdis.eval()\n",
    "\n",
    "    if param['model'] == 'sem':\n",
    "        embed, _ = encoder(features, adj)\n",
    "    else:\n",
    "        embed = encoder(features, adj)\n",
    "    output = classifier(embed, adj)\n",
    "\n",
    "    loss_test = F.cross_entropy(output[idx_test], labels[idx_test])\n",
    "    acc_test, auc_test, macro_F_test, positive_precision, positive_recall, positive_f1_test, conf_matrix = utils.evaluation(output[idx_test], labels[idx_test])\n",
    "\n",
    "    print(\"\\033[0;30;41m [{}] Loss: {}, Accuracy: {:f}, Auc-Roc score: {:f}, Macro-F1 score: {:f}, positive_precision: {:f}, positive_recall: {:f}, positive_f1_score: {:f}\\033[0m\".format(epoch, loss_test.item(), acc_test, auc_test, macro_F_test, positive_precision, positive_recall, positive_f1_test))\n",
    "    print(conf_matrix)\n",
    "    return acc_test, auc_test, macro_F_test\n",
    "\n",
    "\n",
    "def save_model(epoch):\n",
    "    saved_content = {}\n",
    "\n",
    "    saved_content['encoder'] = encoder.state_dict()\n",
    "    saved_content['decoder'] = decoder.state_dict()\n",
    "    saved_content['classifier'] = classifier.state_dict()\n",
    "    saved_content['pairdis'] = pairdis.state_dict()\n",
    "    saved_content['clusterdis'] = clusterdis.state_dict()\n",
    "    \n",
    "    torch.save(saved_content, r'C:\\Users\\George\\Desktop\\ISEF-2023\\Model\\test\\GraphMixup/checkpoint/{}/{}_{}.pth'.format(param['dataset'], param['setting'], epoch))\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    loaded_content = torch.load(r'C:\\Users\\George\\Desktop\\ISEF-2023\\Model\\test\\GraphMixup/checkpoint/{}/{}.pth'.format(param['dataset'], filename), map_location=lambda storage, loc: storage)\n",
    "\n",
    "    encoder.load_state_dict(loaded_content['encoder'])\n",
    "    decoder.load_state_dict(loaded_content['decoder'])\n",
    "    classifier.load_state_dict(loaded_content['classifier'])\n",
    "    pairdis.load_state_dict(loaded_content['pairdis'])\n",
    "    clusterdis.load_state_dict(loaded_content['clusterdis'])\n",
    "\n",
    "    print(\"successfully loaded: \"+ filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nni\\runtime\\trial_command_channel\\standalone.py:34: RuntimeWarning: \u001b[1m\u001b[31mRunning trial code without runtime. Please check the tutorial if you are new to NNI: \u001b[33mhttps://nni.readthedocs.io/en/stable/tutorials/hpo_quickstart_pytorch/main.html\u001b[0m\n",
      "  warnings.warn(warning_message, RuntimeWarning)\n",
      "C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\io\\files.py:295: RuntimeWarning: Could not add vertex ids, there is already an 'id' vertex attribute. at src/io/graphml.c:492\n",
      "  return reader(f, *args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--load', type=str, default=None)\n",
    "parser.add_argument('--dataset', type=str, default='PPI', choices=['cora','BlogCatalog', 'wiki-cs', 'PPI'])\n",
    "parser.add_argument('--im_ratio', type=float, default=0.5)\n",
    "parser.add_argument('--num_im_class', type=int, default=2, choices=[2, 14, 10])\n",
    "\n",
    "parser.add_argument('--model', type=str, default='sem', choices=['sage','gcn', 'sem', 'gat'])\n",
    "parser.add_argument('--setting', type=str, default='pre-train', choices=['raw', 'pre-train', 'fine-tune', 'joint', 'over-sampling', 'smote', 'embed_smote', 're-weight'])\n",
    "parser.add_argument('--mode', type=str, default='continuous_edge', choices=['discrete_edge','continuous_edge'])\n",
    "parser.add_argument('--nhead', type=int, default=4)\n",
    "parser.add_argument('--graph_mode', type=int, default=1)\n",
    "parser.add_argument('--dis_weight', type=float, default=1.0)\n",
    "parser.add_argument('--clu_weight', type=float, default=1.0)\n",
    "parser.add_argument('--up_scale', type=float, default=0)\n",
    "parser.add_argument('--delta_k', type=float, default=0.05)\n",
    "\n",
    "parser.add_argument('--nhid', type=int, default=128)\n",
    "parser.add_argument('--dis_nclass', type=int, default=2)\n",
    "parser.add_argument('--clu_nclass', type=int, default=2)\n",
    "parser.add_argument('--k_num', type=int, default=5000)\n",
    "parser.add_argument('--dropout', type=float, default=0.1)\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=2010)\n",
    "parser.add_argument('--lr', type=float, default=0.001)\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4)\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "param = args.__dict__\n",
    "param.update(nni.get_next_parameter())\n",
    "\n",
    "random.seed(param['seed'])\n",
    "np.random.seed(param['seed'])\n",
    "torch.manual_seed(param['seed'])\n",
    "torch.cuda.manual_seed(param['seed'])\n",
    "\n",
    "if param['dataset'] == 'BlogCatalog':\n",
    "    param['num_im_class'] = 14\n",
    "    param['epochs'] = 4010\n",
    "    param['clu_weight'] = 1e-3\n",
    "    param['dis_nclass'] = 3\n",
    "    param['clu_nclass'] = 30\n",
    "    param['k_num'] = 5000\n",
    "if param['dataset'] == 'wiki-cs':\n",
    "    param['num_im_class'] = 10\n",
    "    param['dis_nclass'] = 5\n",
    "    param['clu_nclass'] = 12\n",
    "    param['k_num'] = 10000\n",
    "    param['dropout'] = 0.5\n",
    "\n",
    "# Load Dataset\n",
    "if param['dataset'] == 'PPI':\n",
    "    idx_train, idx_val, idx_test, adj, features, labels = data_load.load_PPI()\n",
    "elif param['dataset'] == 'cora':\n",
    "    idx_train, idx_val, idx_test, adj, features, labels = data_load.load_cora(num_per_class=20, num_im_class=param['num_im_class'], im_ratio=param['im_ratio'])\n",
    "elif param['dataset'] == 'BlogCatalog':\n",
    "    idx_train, idx_val, idx_test, adj, features, labels = data_load.load_BlogCatalog()\n",
    "elif param['dataset'] == 'wiki-cs':\n",
    "    idx_train, idx_val, idx_test, adj, features, labels = data_load.load_wiki_cs()\n",
    "else:\n",
    "    print(\"no this dataset: {param['dataset']}\")\n",
    "\n",
    "# For over-sampling and smote methods, they directly upsampling data in the input space\n",
    "if param['setting'] == 'over-sampling':\n",
    "    features, labels, idx_train, adj = utils.src_upsample(features, labels, idx_train, adj, up_scale=param['up_scale'], im_class_num=param['num_im_class'])\n",
    "if param['setting'] == 'smote':\n",
    "    features, labels, idx_train, adj = utils.src_smote(features, labels, idx_train, adj, up_scale=param['up_scale'], im_class_num=param['num_im_class'])\n",
    "\n",
    "# Load different bottleneck encoders and classifiers\n",
    "if param['setting'] != 'embed_smote':\n",
    "    if param['model'] == 'sage':\n",
    "        encoder = models.Sage_En(nfeat=features.shape[1], nhid=param['nhid'], nembed=param['nhid'], dropout=param['dropout'])\n",
    "        classifier = models.Sage_Classifier(nembed=param['nhid'], nhid=param['nhid'], nclass=labels.max().item() + 1, dropout=param['dropout'])\n",
    "    elif param['model'] == 'gcn':\n",
    "        encoder = models.GCN_En(nfeat=features.shape[1], nhid=param['nhid'], nembed=param['nhid'], dropout=param['dropout'])\n",
    "        classifier = models.GCN_Classifier(nembed=param['nhid'], nhid=param['nhid'], nclass=labels.max().item() + 1, dropout=param['dropout'])\n",
    "    elif args.model == 'sem':\n",
    "        encoder = models.SEM_En(nfeat=features.shape[1], nhid=param['nhid'], nembed=param['nhid'], dropout=param['dropout'], nheads=param['nhead'], graph_mode=param['graph_mode'])\n",
    "        classifier = models.SEM_Classifier(nembed=args.nhid, nhid=param['nhid'], nclass=labels.max().item() + 1, dropout=param['dropout'])\n",
    "    elif args.model == 'gat':\n",
    "        encoder = models.GAT_En(nfeat=features.shape[1], nhid=param['nhid'], nembed=param['nhid'], dropout=param['dropout'], nheads=param['nhead'])\n",
    "        classifier = models.GAT_Classifier(nembed=args.nhid, nhid=param['nhid'], nclass=labels.max().item() + 1, dropout=param['dropout'], nheads=param['nhead'])\n",
    "else:\n",
    "    if args.model == 'sage':\n",
    "        encoder = models.Sage_En2(nfeat=features.shape[1], nhid=param['nhid'], nembed=param['nhid'], dropout=param['dropout'])\n",
    "        classifier = models.Classifier(nembed=args.nhid, nhid=param['nhid'], nclass=labels.max().item() + 1, dropout=param['dropout'])\n",
    "    elif args.model == 'gcn':\n",
    "        encoder = models.GCN_En2(nfeat=features.shape[1], nhid=param['nhid'], nembed=param['nhid'], dropout=param['dropout'])\n",
    "        classifier = models.Classifier(nembed=args.nhid, nhid=param['nhid'], nclass=labels.max().item() + 1, dropout=param['dropout'])       \n",
    "    elif args.model == 'sem':\n",
    "        encoder = models.SEM_En2(nfeat=features.shape[1], nhid=param['nhid'], nembed=param['nhid'], dropout=param['dropout'], nheads=param['nhead'], graph_mode=param['graph_mode'])\n",
    "        classifier = models.Classifier(nembed=args.nhid, nhid=param['nhid'], nclass=labels.max().item() + 1, dropout=param['dropout'])\n",
    "    elif args.model == 'gat':\n",
    "        encoder = models.GAT_En2(nfeat=features.shape[1], nhid=param['nhid'], nembed=param['nhid'], dropout=param['dropout'], nheads=param['nhead'])\n",
    "        classifier = models.Classifier(nembed=args.nhid, nhid=param['nhid'], nclass=labels.max().item() + 1, dropout=param['dropout'])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metis graph clustering started ...\n",
      "Number of nodes in clusters: {0: 3696, 1: 3696}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\George\\Desktop\\ISEF-2023\\Model\\test\\GraphMixup\\models.py:127: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:607.)\n",
      "  new_adj = torch.sparse.FloatTensor(edge_list, e, torch.Size([h.shape[0], h.shape[0]])).to_dense()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0]) 1\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Run training and testing for maximum epochs with early stopping\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 46\u001b[0m     f1_val \u001b[38;5;241m=\u001b[39m train(epoch)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     49\u001b[0m         acc_test, roc_test, macrof1_test, positive_precision, positive_recall, positive_f1, conf_matrix \u001b[38;5;241m=\u001b[39m test(epoch)\n",
      "Cell \u001b[1;32mIn [2], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     24\u001b[0m     embed \u001b[38;5;241m=\u001b[39m encoder(features, adj)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetting\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoint\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetting\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre-train\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetting\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfine-tune\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Feature Mixup, Label Mixup, and Edge Mixup in the semantic relation space\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     embed, labels_new, idx_train_new, adj_up \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixup\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mup_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mup_scale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim_class_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_im_class\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     n_num \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     31\u001b[0m     adj_rec \u001b[38;5;241m=\u001b[39m decoder(embed)\n",
      "File \u001b[1;32mc:\\Users\\George\\Desktop\\ISEF-2023\\Model\\test\\GraphMixup\\utils.py:191\u001b[0m, in \u001b[0;36mmixup\u001b[1;34m(embed, labels, idx_train, adj, up_scale, im_class_num, scale)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mprint\u001b[39m(chosen\u001b[38;5;241m.\u001b[39mshape, c_largest\u001b[38;5;241m-\u001b[39mi)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m up_scale \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 191\u001b[0m     c_up_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mavg_number\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchosen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m scale) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c_up_scale \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    193\u001b[0m         up_scale_rest \u001b[38;5;241m=\u001b[39m avg_number\u001b[38;5;241m/\u001b[39mchosen\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m scale \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m c_up_scale\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Load edge predictor and modules for Local-Path and Global-Path Prediction\n",
    "decoder = models.Decoder(nembed=param['nhid'], dropout=param['dropout'])\n",
    "pairdis = models.PairwiseDistance(nhid=param['nhid'], adj=adj, device=device, param=param)\n",
    "clusterdis = models.DistanceCluster(nhid=param['nhid'], adj=adj, features=features, device=device, param=param)\n",
    "\n",
    "# Load three optimizer for the semantic feature extractor, edge predictor, and node classifier\n",
    "optimizer_en = torch.optim.Adam(encoder.parameters(), lr=param['lr'], weight_decay=param['weight_decay'])\n",
    "optimizer_cls = torch.optim.Adam(classifier.parameters(), lr=param['lr'], weight_decay=param['weight_decay'])\n",
    "optimizer_de = torch.optim.Adam(list(decoder.parameters()) + list(pairdis.parameters()) + list(clusterdis.parameters()), lr=param['lr'], weight_decay=param['weight_decay'])\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "classifier = classifier.to(device)\n",
    "decoder = decoder.to(device)\n",
    "pairdis = pairdis.to(device)\n",
    "clusterdis = clusterdis.to(device)\n",
    "\n",
    "features = features.to(device)\n",
    "adj = adj.to(device)\n",
    "labels = labels.to(device)\n",
    "idx_train = idx_train.to(device)\n",
    "idx_val = idx_val.to(device)\n",
    "idx_test = idx_test.to(device)\n",
    "\n",
    "if param['load'] is not None:\n",
    "    load_model(param['load'])\n",
    "\n",
    "# Initialize the RL agent\n",
    "env = QLearning.GNN_env(action_value=0.05)\n",
    "RL = QLearning.QLearningTable(actions=list(range(env.n_actions)))\n",
    "\n",
    "last_k = 0.0\n",
    "current_k = 0.0\n",
    "last_acc = 0.0\n",
    "current_acc = 0.0\n",
    "action = None\n",
    "k_record = [0]\n",
    "Endepoch = 0\n",
    "\n",
    "es = 0\n",
    "f1_val_best = 0\n",
    "metric_test_val = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "metric_test_best = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# Run training and testing for maximum epochs with early stopping\n",
    "for epoch in range(param['epochs']):\n",
    "    f1_val = train(epoch)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        acc_test, roc_test, macrof1_test, positive_precision, positive_recall, positive_f1, conf_matrix = test(epoch)\n",
    "        if f1_val > f1_val_best:\n",
    "            f1_val_best = f1_val\n",
    "            metric_test_val[0] = acc_test\n",
    "            metric_test_val[1] = roc_test\n",
    "            metric_test_val[2] = macrof1_test\n",
    "            metric_test_val[3] = epoch\n",
    "            metric_test_val[4] = positive_precision\n",
    "            metric_test_val[5] = positive_recall\n",
    "            metric_test_val[6] = positive_f1\n",
    "            metric_test_val[7] = conf_matrix\n",
    "            es = 0\n",
    "        elif param['setting'] == 'fine-tune':\n",
    "            es += 1\n",
    "            if es >= 20:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "        if macrof1_test > metric_test_best[2]:\n",
    "            metric_test_best[0] = acc_test\n",
    "            metric_test_best[1] = roc_test\n",
    "            metric_test_best[2] = macrof1_test\n",
    "            metric_test_best[3] = positive_precision\n",
    "            metric_test_best[4] = positive_recall\n",
    "            metric_test_best[5] = positive_f1\n",
    "\n",
    "    if epoch % 500 == 0 and param['setting'] == 'pre-train':\n",
    "        save_model(epoch)\n",
    "\n",
    "\n",
    "if param['setting'] == 'pre-train':\n",
    "    param['setting'] = 'fine-tune'\n",
    "\n",
    "    es = 0\n",
    "    f1_val_best = 0\n",
    "    metric_test_val = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    metric_test_best = [0, 0, 0, 0, 0, 0]\n",
    "    for epoch in range(param['epochs']):\n",
    "        f1_val = train(epoch)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            acc_test, roc_test, macrof1_test, positive_precision, positive_recall, positive_f1, conf_matrix = test(epoch)\n",
    "            if f1_val > f1_val_best:\n",
    "                f1_val_best = f1_val\n",
    "                metric_test_val[0] = acc_test\n",
    "                metric_test_val[1] = roc_test\n",
    "                metric_test_val[2] = macrof1_test\n",
    "                metric_test_val[3] = epoch\n",
    "                metric_test_val[4] = positive_precision\n",
    "                metric_test_val[5] = positive_recall\n",
    "                metric_test_val[6] = positive_f1\n",
    "                metric_test_val[7] = conf_matrix\n",
    "                es = 0\n",
    "            else:\n",
    "                es += 1\n",
    "                if es >= 20:\n",
    "                    print(\"Early stopping!\")\n",
    "                    break\n",
    "\n",
    "            if macrof1_test > metric_test_best[2]:\n",
    "                metric_test_best[0] = acc_test\n",
    "                metric_test_best[1] = roc_test\n",
    "                metric_test_best[2] = macrof1_test\n",
    "                metric_test_best[3] = positive_precision\n",
    "                metric_test_best[4] = positive_recall\n",
    "                metric_test_best[5] = positive_f1\n",
    "\n",
    "# Save all classification results\n",
    "nni.report_final_result(metric_test_val[2])\n",
    "outFile = open(r'C:\\Users\\George\\Desktop\\ISEF-2023\\Model\\test\\GraphMixup/PerformMetrics_{}.csv'.format(param['dataset']),'a+', newline='')\n",
    "writer = csv.writer(outFile, dialect='excel')\n",
    "results = [time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())]\n",
    "for v, k in param.items():\n",
    "    results.append(k)\n",
    "results.append(str(f1_val_best))\n",
    "results.append(str(metric_test_val[0]))\n",
    "results.append(str(metric_test_val[1]))\n",
    "results.append(str(metric_test_val[2]))\n",
    "results.append(str(metric_test_val[3]))\n",
    "results.append(str(metric_test_val[4]))\n",
    "results.append(str(metric_test_val[5]))\n",
    "results.append(str(metric_test_val[6]))\n",
    "results.append(str(metric_test_val[7]))\n",
    "results.append(str(metric_test_best[0]))\n",
    "results.append(str(metric_test_best[1]))\n",
    "results.append(str(metric_test_best[2]))\n",
    "results.append(str(metric_test_best[3]))\n",
    "results.append(str(metric_test_best[4]))\n",
    "results.append(str(metric_test_best[5]))\n",
    "\n",
    "results.append(Endepoch)\n",
    "results.append(k_record[-1])\n",
    "writer.writerow(results)\n",
    "\n",
    "# np.save(\"../result/{}/RL_process_{}.npy\".format(param['dataset'], Endepoch), np.array(k_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
